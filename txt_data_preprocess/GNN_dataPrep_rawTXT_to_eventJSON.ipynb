{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3057b315-2354-4178-a64d-7519678be7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import math\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca3bfb37-d7dd-442e-a86d-a76006e71eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath: str, sep: str = '\\t') -> pd.DataFrame | None:\n",
    "    \"\"\"\n",
    "    Loads data from a text file (CSV format) into a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The path to the text file.\n",
    "        sep (str): Delimiter to use.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame | None: DataFrame containing the loaded data, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Loading data from {filepath}...\")\n",
    "        df = pd.read_csv(filepath, sep=sep, low_memory=False)\n",
    "        print(f\"--- Data loaded successfully: {df.shape[0]} events, {df.shape[1]} columns :)\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"*** Error: File not found at {filepath}\")\n",
    "        return None\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"*** Error: File at {filepath} is empty.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"*** An unexpected error occurred during file loading: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def save_to_json(data: list[dict], filepath: str) -> bool:\n",
    "    \"\"\"\n",
    "    Saves a list of dictionaries to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        data (list): The list of event dictionaries.\n",
    "        filepath (str): The path where the JSON file will be saved.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if saving was successful, False otherwise.\n",
    "    \"\"\"\n",
    "    if not isinstance(data, list):\n",
    "        print(\"Error: Data to be saved must be a list of dictionaries.\")\n",
    "        return False\n",
    "    if not data:\n",
    "        print(\"Warning: Data list is empty. Saving an empty JSON file.\")\n",
    "        return False\n",
    "\n",
    "    print(f\"Attempting to save {len(data)} events to JSON file: {filepath}\")\n",
    "    try:\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(data, f, indent=1)\n",
    "        print(\"JSON file saved successfully.\")\n",
    "        return True\n",
    "    except TypeError as e:\n",
    "        print(f\"Error: Data contains types not serializable to JSON: {e}\")\n",
    "        print(\"This might indicate NumPy types weren't converted or other complex objects exist.\")\n",
    "        return False\n",
    "    except IOError as e:\n",
    "        print(f\"Error: Could not write to file {filepath}: {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during JSON saving: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "def filter_zero_multiplicity(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes events with zero jetmultiplicity.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame with event data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with zero jetmultiplicity events removed.\n",
    "                     Returns None if the input DataFrame is invalid or lacks\n",
    "                     the 'jetmultiplicity' column.\n",
    "    \"\"\"\n",
    "    if df is None or not isinstance(df, pd.DataFrame):\n",
    "        print(\"*** Error: Invalid input DataFrame :(\")\n",
    "        return None\n",
    "    if 'jetmultiplicity' not in df.columns:\n",
    "        print(\"*** Error: 'jetmultiplicity' column not found in DataFrame :(\")\n",
    "        return None\n",
    "\n",
    "    initial_events = len(df)\n",
    "    print(f\"Initial number of events: {initial_events}\")\n",
    "\n",
    "    # Filter events where jetmultiplicity is greater than 0\n",
    "    df_filtered = df[df['jetmultiplicity'] > 0].copy()\n",
    "\n",
    "    removed_events = initial_events - len(df_filtered)\n",
    "    print(f\"--- Removed {removed_events} events with zero jetmultiplicity :)\")\n",
    "    print(f\"Number of events after filtering: {len(df_filtered)}\")\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "def filter_jets_by_eta(df:pd.DataFrame, eta_min:float=-2.5, eta_max:float=2.5, max_jets:int=13) -> [pd.DataFrame, list]:\n",
    "    \"\"\"\n",
    "    Sets jet quantities to NaN if the jet's Eta is outside the specified range.\n",
    "\n",
    "    It iterates through each possible jet (1 to max_jets) and checks its Eta value.\n",
    "    If Eta is outside [eta_min, eta_max], all features (Eta, Phi, pT, Px, Py, Pz, E)\n",
    "    for that specific jet in that event are set to NaN.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame with event data.\n",
    "        eta_min (float): The minimum allowed Eta value. Defaults to -2.5.\n",
    "        eta_max (float): The maximum allowed Eta value. Defaults to 2.5.\n",
    "        max_jets (int): The maximum number of jets to check per event. Defaults to 13.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with jet quantities potentially modified to NaN.\n",
    "                      Returns None if the input DataFrame is invalid.\n",
    "    \"\"\"\n",
    "    if df is None or not isinstance(df, pd.DataFrame):\n",
    "        print(\"*** Error: Invalid input DataFrame for Eta filtering :(\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Applying Eta filter: Keeping jets with Eta between {eta_min} and {eta_max}.\")\n",
    "\n",
    "    df_modified = df.copy()\n",
    "    jet_eta_cols_in_df = []\n",
    "    for i in range(1, max_jets + 1):\n",
    "        eta_col = f'jet{i}_Eta'\n",
    "        if eta_col in df_modified.columns:\n",
    "            jet_eta_cols_in_df.append(eta_col)\n",
    "\n",
    "            # mask = ~df_modified[eta_col].between(eta_min, eta_max, inclusive='both')\n",
    "            # jet_cols = [f'jet{i}_{feature}' for feature in JET_FEATURES]\n",
    "            # existing_jet_cols = [col for col in jet_cols if col in df_modified.columns]\n",
    "            # if not existing_jet_cols:\n",
    "            #     continue\n",
    "            # df_modified.loc[mask, existing_jet_cols] = np.nan\n",
    "    print(\"--- Eta filtering complete :)\")\n",
    "\n",
    "    return df_modified, jet_eta_cols_in_df\n",
    "\n",
    "\n",
    "def filter_empty_events(df:pd.DataFrame, jet_eta_cols:list, max_photons:int=3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes events that have no valid jets AND no valid photons after processing.\n",
    "\n",
    "    - No valid jets means all existing jet_Eta columns for the event are NaN.\n",
    "    - No valid photons means all existing isophoton_E columns are <= 0 (or NaN).\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame after jet Eta filtering.\n",
    "        jet_eta_cols (list): List of jet_Eta column names that actually exist in df.\n",
    "        max_photons (int): Maximum number of photons to check.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with empty events removed, or None if input is invalid.\n",
    "    \"\"\"\n",
    "    if df is None or not isinstance(df, pd.DataFrame):\n",
    "        print(\"*** Error: Invalid input DataFrame for empty event filtering :(\")\n",
    "        return None\n",
    "    if not jet_eta_cols:\n",
    "         print(\"*** Warning: No jet Eta columns found in DataFrame. Cannot filter based on jets :(\")\n",
    "         has_no_valid_jets = pd.Series([True] * len(df), index=df.index) # Assume no jets if no columns\n",
    "    else:\n",
    "        # Check rows where ALL existing jet_Eta columns are NaN\n",
    "        has_no_valid_jets = df[jet_eta_cols].isnull().all(axis=1)\n",
    "\n",
    "    photon_e_cols = [f'isophoton{i}_E' for i in range(1, max_photons + 1)]\n",
    "    photon_e_cols_in_df = [col for col in photon_e_cols if col in df.columns]\n",
    "\n",
    "    if not photon_e_cols_in_df:\n",
    "        print(\"*** Warning: No photon Energy columns found in DataFrame. Cannot filter based on photons :(\")\n",
    "        has_no_valid_photons = pd.Series([True] * len(df), index=df.index) # Assume no photons if no columns\n",
    "    else:\n",
    "        has_no_valid_photons = (df[photon_e_cols_in_df].fillna(0) <= 0).all(axis=1)\n",
    "\n",
    "    # Identify events to remove (those having no valid jets AND no valid photons)\n",
    "    is_empty_event = has_no_valid_jets & has_no_valid_photons\n",
    "\n",
    "    # Filter the DataFrame: keep rows where is_empty_event is False\n",
    "    df_filtered = df[~is_empty_event].copy()\n",
    "\n",
    "    removed_count = len(df) - len(df_filtered)\n",
    "    if removed_count > 0:\n",
    "        print(f\"Removed {removed_count} events with no valid jets AND no valid photons :)\")\n",
    "    else:\n",
    "        print(\"No events found with both empty jets and empty photons.\")\n",
    "    print(f\"Number of events after empty event filtering: {len(df_filtered)}\")\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "def save_data(df:pd.DataFrame, output_filepath:str) -> bool:\n",
    "    \"\"\"\n",
    "    Saves the DataFrame to a txt file.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to save.\n",
    "        output_filepath (str): The path where the txt file will be saved.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if saving was successful, False otherwise.\n",
    "    \"\"\"\n",
    "    if df is None or not isinstance(df, pd.DataFrame):\n",
    "        print(\"*** Error: Invalid DataFrame provided for saving :(\")\n",
    "        return False\n",
    "    try:\n",
    "        print(f\"Saving processed data to {output_filepath}...\")\n",
    "        df.to_csv(output_filepath, index=False, sep = '\\t')\n",
    "        print(\"--- Data saved successfully :)\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"*** An unexpected error occurred during file saving: {e}\")\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "446b2c73-7c05-4cd0-ac23-f3d110354b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Calculation Helper Functions ---\n",
    "\n",
    "def calculate_delta_r_robust(eta1: float | None, phi1: float | None,\n",
    "                             eta2: float | None, phi2: float | None) -> float:\n",
    "    \"\"\"\n",
    "    Compute the Delta R distance between two nodes using (Eta, Phi).\n",
    "    Handles potential None inputs and periodicity in Phi.\n",
    "\n",
    "    Returns:\n",
    "    - float: The Delta R distance, or np.nan if inputs are invalid.\n",
    "    \"\"\"\n",
    "    if any(v is None for v in [eta1, phi1, eta2, phi2]):\n",
    "        return np.nan\n",
    "    try:\n",
    "        eta1_f, phi1_f, eta2_f, phi2_f = map(float, [eta1, phi1, eta2, phi2])\n",
    "        deta = eta1_f - eta2_f\n",
    "        dphi = phi1_f - phi2_f\n",
    "        # map dphi between [-pi, pi]\n",
    "        dphi = np.arctan2(np.sin(dphi), np.cos(dphi))\n",
    "\n",
    "        delta_r_sq = deta**2 + dphi**2\n",
    "        # Ensure result is not complex due to tiny numerical errors\n",
    "        if isinstance(delta_r_sq, complex) or delta_r_sq < 0:\n",
    "             return np.nan\n",
    "\n",
    "        result = np.sqrt(delta_r_sq)\n",
    "        return result if not np.isnan(result) else np.nan\n",
    "    except (TypeError, ValueError):\n",
    "        return np.nan\n",
    "\n",
    "def calculate_invariant_mass_robust(four_vectors: list[list[float]]) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the invariant mass of a system given a list of four-vectors [E, Px, Py, Pz].\n",
    "\n",
    "    Args:\n",
    "        four_vectors (list[list[float]]): List where each inner list is [E, Px, Py, Pz].\n",
    "\n",
    "    Returns:\n",
    "        float: The invariant mass, or np.nan if calculation is not possible.\n",
    "    \"\"\"\n",
    "    if not four_vectors:\n",
    "        return np.nan\n",
    "\n",
    "    try:\n",
    "        fv_array = np.array(four_vectors, dtype=float)\n",
    "\n",
    "        if fv_array.ndim != 2 or fv_array.shape[1] != 4 or np.isnan(fv_array).any():\n",
    "            return np.nan\n",
    "\n",
    "        # Summing along the particle axis (axis=0)\n",
    "        sum_fv = np.sum(fv_array, axis=0)\n",
    "        tot_E, tot_Px, tot_Py, tot_Pz = sum_fv\n",
    "\n",
    "        # Check if sum resulted in NaN (unlikely if individual checks passed, but safe)\n",
    "        if np.isnan(sum_fv).any():\n",
    "            return np.nan\n",
    "\n",
    "        mass_squared = tot_E**2 - (tot_Px**2 + tot_Py**2 + tot_Pz**2)\n",
    "\n",
    "        # Check for NaN or negative mass squared (allowing small tolerance)\n",
    "        if np.isnan(mass_squared) or mass_squared < -1e-9: # Tolerance for numerical precision\n",
    "            return np.nan\n",
    "        if mass_squared < 0:\n",
    "             mass_squared = 0.0 # Treat slightly negative as zero\n",
    "\n",
    "        result = np.sqrt(mass_squared)\n",
    "        return result if not np.isnan(result) else np.nan\n",
    "\n",
    "    except (TypeError, ValueError, IndexError):\n",
    "        return np.nan\n",
    "\n",
    "# --- Core Event Processing Function ---\n",
    "\n",
    "def process_event_to_gnn(event_series: pd.Series, event_label: int,\n",
    "                         max_jets: int, max_photons: int) -> dict | None:\n",
    "    \"\"\"\n",
    "    Processes a single event (represented as a Pandas Series)\n",
    "    and converts it into the GNN dictionary format.\n",
    "\n",
    "    Args:\n",
    "        event_series (pd.Series): A row from the DataFrame representing one event.\n",
    "        event_label (int): The predetermined label (0 or 1) for this event.\n",
    "        max_jets (int): Maximum number of potential jet columns (e.g., jet1_, ..., jetN_).\n",
    "        max_photons (int): Maximum number of potential photon columns.\n",
    "\n",
    "    Returns:\n",
    "        dict | None: A dictionary in GNN format, or None if processing fails.\n",
    "    \"\"\"\n",
    "    event_no = event_series.get('eventno', None)\n",
    "    if event_no is None:\n",
    "        # print(\"Warning: Event skipped, 'eventno' column missing or NaN.\")\n",
    "        return None\n",
    "    try:\n",
    "        event_no = int(event_no) # Ensure event number is integer\n",
    "    except (ValueError, TypeError):\n",
    "        # print(f\"Warning: Event skipped, could not convert eventno '{event_no}' to int.\")\n",
    "        return None\n",
    "\n",
    "    nodes = []\n",
    "    node_positions = [] # Store [Eta, Phi] separately for edge calculation\n",
    "    node_labels = [] # 0 for photon, 1 for jet (conventional)\n",
    "    jet_btag_labels_for_nodes = [] # Aligned with nodes: btag value for jets, 0 for photons\n",
    "    particle_four_vectors = [] # Store [E, Px, Py, Pz] for valid nodes\n",
    "\n",
    "    # --- Extract Photons ---\n",
    "    for i in range(1, max_photons + 1):\n",
    "        # Construct column names efficiently\n",
    "        p_prefix = f'isophoton{i}_'\n",
    "        p_node_cols = [p_prefix + feat for feat in NODE_FEATURES]\n",
    "        p_pos_cols = [p_prefix + feat for feat in POS_FEATURES]\n",
    "        p_kin_cols = [p_prefix + feat for feat in KINEMATIC_FEATURES]\n",
    "\n",
    "        # Check if essential columns exist (more robust than checking value sum)\n",
    "        if not all(col in event_series.index for col in p_node_cols):\n",
    "            continue # Skip if this photon's columns don't fully exist\n",
    "\n",
    "        try:\n",
    "            # Extract features, convert to float, check for NaN\n",
    "            node_vals = [float(event_series.get(col, np.nan)) for col in p_node_cols]\n",
    "            pos_vals = [float(event_series.get(col, np.nan)) for col in p_pos_cols]\n",
    "            kin_vals = [float(event_series.get(col, np.nan)) for col in p_kin_cols]\n",
    "\n",
    "            # Use pT > 0 as the primary check for photon existence/validity\n",
    "            # Assuming NODE_FEATURES = ['Eta', 'Phi', 'pT', 'E'], pT is index 2\n",
    "            if len(node_vals) == len(NODE_FEATURES) and not np.isnan(node_vals[2]) and node_vals[2] > 0:\n",
    "                 # Check if *all* extracted values for this particle are valid numbers\n",
    "                if not np.isnan(node_vals).any() and not np.isnan(pos_vals).any() and not np.isnan(kin_vals).any():\n",
    "                    nodes.append(node_vals)\n",
    "                    node_positions.append(pos_vals)\n",
    "                    node_labels.append(0) # Photon label = 0\n",
    "                    jet_btag_labels_for_nodes.append(0) # B-tag is NaN for photons\n",
    "                    particle_four_vectors.append(kin_vals)\n",
    "                # else: # Optional: Warn if photon has pT>0 but other NaNs\n",
    "                    # print(f\"Warning: Event {event_no}, Photon {i} - pT>0 but NaN found in features. Skipping node.\")\n",
    "\n",
    "        except (TypeError, ValueError, KeyError):\n",
    "            # print(f\"Warning: Event {event_no}, Photon {i} - Error extracting/converting features. Skipping.\")\n",
    "            continue # Skip this photon if any error occurs\n",
    "\n",
    "    # --- Extract Jets ---\n",
    "    jets_for_mass_calc = [] # Store full jet dicts needed later\n",
    "    for i in range(1, max_jets + 1):\n",
    "        j_prefix = f'jet{i}_'\n",
    "        j_node_cols = [j_prefix + feat for feat in NODE_FEATURES]\n",
    "        j_pos_cols = [j_prefix + feat for feat in POS_FEATURES]\n",
    "        j_kin_cols = [j_prefix + feat for feat in KINEMATIC_FEATURES]\n",
    "        j_btag_col = j_prefix + BTAG_FEATURE\n",
    "\n",
    "        # Check if essential columns exist\n",
    "        if not all(col in event_series.index for col in j_node_cols) or j_btag_col not in event_series.index:\n",
    "             continue\n",
    "\n",
    "        try:\n",
    "            node_vals = [float(event_series.get(col, np.nan)) for col in j_node_cols]\n",
    "            pos_vals = [float(event_series.get(col, np.nan)) for col in j_pos_cols]\n",
    "            kin_vals = [float(event_series.get(col, np.nan)) for col in j_kin_cols]\n",
    "            btag_val_raw = event_series.get(j_btag_col, np.nan)\n",
    "            btag_val = float(btag_val_raw) if not pd.isna(btag_val_raw) else np.nan\n",
    "\n",
    "            # Use pT > 0 primary check for jet validity\n",
    "            if len(node_vals) == len(NODE_FEATURES) and not np.isnan(node_vals[2]) and node_vals[2] > 0:\n",
    "                 # Check if all extracted values are valid numbers (including btag)\n",
    "                if not np.isnan(node_vals).any() and not np.isnan(pos_vals).any() and not np.isnan(kin_vals).any() and not np.isnan(btag_val):\n",
    "                    nodes.append(node_vals)\n",
    "                    node_positions.append(pos_vals)\n",
    "                    node_labels.append(1) # Jet label = 1\n",
    "                    jet_btag_labels_for_nodes.append(btag_val)\n",
    "                    particle_four_vectors.append(kin_vals)\n",
    "                    # Store jet info needed for later mass calculation (original dict structure preferred by invariant mass func)\n",
    "                    jets_for_mass_calc.append({\n",
    "                        'E': kin_vals[0], 'Px': kin_vals[1], 'Py': kin_vals[2], 'Pz': kin_vals[3],\n",
    "                        'pT': node_vals[2] # Include pT for sorting\n",
    "                    })\n",
    "                # else:\n",
    "                    # print(f\"Warning: Event {event_no}, Jet {i} - pT>0 but NaN found in features/btag. Skipping node.\")\n",
    "\n",
    "        except (TypeError, ValueError, KeyError):\n",
    "             # print(f\"Warning: Event {event_no}, Jet {i} - Error extracting/converting features. Skipping.\")\n",
    "             continue\n",
    "\n",
    "    # --- Check if any valid nodes were created ---\n",
    "    num_nodes = len(nodes)\n",
    "    if num_nodes == 0:\n",
    "        # print(f\"Info: Event {event_no} resulted in 0 valid nodes. Skipping event.\")\n",
    "        return None\n",
    "\n",
    "    # --- Calculate Edges and Edge Index ---\n",
    "    edge_index_sources = []\n",
    "    edge_index_targets = []\n",
    "    edges = [] # DeltaR values\n",
    "\n",
    "    if num_nodes >= 2:\n",
    "        for i in range(num_nodes):\n",
    "            for j in range(i + 1, num_nodes): # Calculate only for j > i\n",
    "                eta1, phi1 = node_positions[i]\n",
    "                eta2, phi2 = node_positions[j]\n",
    "                delta_r = calculate_delta_r_robust(eta1, phi1, eta2, phi2)\n",
    "\n",
    "                if not np.isnan(delta_r):\n",
    "                    # Add edges in both directions for undirected graph\n",
    "                    edge_index_sources.extend([i, j])\n",
    "                    edge_index_targets.extend([j, i])\n",
    "                    edges.extend([delta_r, delta_r]) # Add distance twice\n",
    "\n",
    "    edge_index = [edge_index_sources, edge_index_targets]\n",
    "\n",
    "    # --- Calculate Graph-Level Features ---\n",
    "    inv_mass_2j = np.nan\n",
    "    inv_mass_2j1p = np.nan\n",
    "    isophoton_pt = np.nan\n",
    "\n",
    "    # Find leading photon (if any) from the 'nodes' list (index 2 is pT)\n",
    "    photon_nodes = [(nodes[i], particle_four_vectors[i]) for i, label in enumerate(node_labels) if label == 0]\n",
    "    if photon_nodes:\n",
    "        photons_sorted = sorted(photon_nodes, key=lambda p: p[0][2], reverse=True) # Sort by pT\n",
    "        leading_photon_fv = photons_sorted[0][1] # Get four-vector [E,Px,Py,Pz]\n",
    "        isophoton_pt = photons_sorted[0][0][2] # Get pT\n",
    "    else:\n",
    "        leading_photon_fv = None\n",
    "\n",
    "    # Find leading 2 jets (if any) using the stored jet dicts\n",
    "    if len(jets_for_mass_calc) >= 2:\n",
    "         # Sort the collected jets by pT\n",
    "        jets_sorted = sorted(jets_for_mass_calc, key=lambda j: j['pT'], reverse=True)\n",
    "        leading_jets_dicts = jets_sorted[:2]\n",
    "        # Extract four-vectors for mass calculation\n",
    "        leading_jets_fv = [[j['E'], j['Px'], j['Py'], j['Pz']] for j in leading_jets_dicts]\n",
    "        inv_mass_2j = calculate_invariant_mass_robust(leading_jets_fv)\n",
    "\n",
    "        # Calculate 2j+1p mass if photon also exists\n",
    "        if leading_photon_fv is not None:\n",
    "            inv_mass_2j1p = calculate_invariant_mass_robust(leading_jets_fv + [leading_photon_fv])\n",
    "\n",
    "    # --- Assemble Final GNN Dictionary ---\n",
    "    # Convert NumPy types (like np.nan) to standard types for JSON\n",
    "    final_nodes = [node.tolist() if isinstance(node, np.ndarray) else node for node in nodes]\n",
    "    final_edges = [edge if not np.isnan(edge) else None for edge in edges]\n",
    "    final_edge_index = edge_index # Already list of lists\n",
    "    final_node_labels = node_labels # Already list of ints\n",
    "    final_jet_btag = [btag if not np.isnan(btag) else None for btag in jet_btag_labels_for_nodes]\n",
    "    num_nodes = len(final_nodes)\n",
    "    num_btag_jets = final_jet_btag.count(1.0)\n",
    "    num_isophotons = final_node_labels.count(0.0)\n",
    "\n",
    "    gnn_dict = {\n",
    "        'eventno': event_no,\n",
    "        'event_label': event_label,\n",
    "        'nodes': final_nodes,\n",
    "        'num_nodes':int(num_nodes) if not np.isnan(num_nodes) else None,\n",
    "        'edges': final_edges,\n",
    "        'edge_index': final_edge_index,\n",
    "        'node_labels': final_node_labels,\n",
    "        'jet_btag_label': final_jet_btag,\n",
    "        'num_btag_jets': int(num_btag_jets) if not np.isnan(num_btag_jets) else None,\n",
    "        'num_isophotons': int(num_isophotons) if not np.isnan(num_isophotons) else None,\n",
    "        'inv_mass_2j1p': float(inv_mass_2j1p) if not np.isnan(inv_mass_2j1p) else None,\n",
    "        'inv_mass_2j': float(inv_mass_2j) if not np.isnan(inv_mass_2j) else None,\n",
    "        'isophoton_pT': float(isophoton_pt) if not np.isnan(isophoton_pt) else None\n",
    "    }\n",
    "    return gnn_dict\n",
    "\n",
    "\n",
    "# --- Main Pipeline Function ---\n",
    "def main_pipeline(input_filepath: str, output_filepath: str,\n",
    "                  MAX_JETS: int, MAX_PHOTONS: int, \n",
    "                  ETA_MIN: float, ETA_MAX: float, sep: str = ',') -> None:\n",
    "    \"\"\"\n",
    "    Runs the full data pipeline: load, process events, save GNN data.\n",
    "\n",
    "    Args:\n",
    "        input_filepath (str): Path to the input data file (CSV/TXT).\n",
    "        output_filepath (str): Path to save the output GNN JSON file.\n",
    "        max_jets (int): Max number of potential jet columns in input.\n",
    "        max_photons (int): Max number of potential photon columns in input.\n",
    "        sep (str): Separator for the input file.\n",
    "    \"\"\"\n",
    "    # 1. Load Data\n",
    "    raw_df = load_data(input_filepath, sep=sep)\n",
    "    if raw_df is not None:\n",
    "        df_filtered_multiplicity = raw_df\n",
    "\n",
    "        if df_filtered_multiplicity is not None:\n",
    "            # 3. Filter jets based on Eta range (Sets invalid jets to NaN)\n",
    "            df_eta_filtered, existing_jet_eta_cols = filter_jets_by_eta(df_filtered_multiplicity,\n",
    "                                                                        eta_min=ETA_MIN,\n",
    "                                                                        eta_max=ETA_MAX,\n",
    "                                                                        max_jets=MAX_JETS)\n",
    "            print(existing_jet_eta_cols)\n",
    "\n",
    "            if df_eta_filtered is not None and not df_eta_filtered.empty:\n",
    "                # 4. Filter out events with no valid jets AND no valid photons\n",
    "                df = filter_empty_events(df_eta_filtered,\n",
    "                                         jet_eta_cols=existing_jet_eta_cols,\n",
    "                                         max_photons=MAX_PHOTONS)\n",
    "\n",
    "            elif df_eta_filtered is not None and df_eta_filtered.empty:\n",
    "                 print(\"*** All events were removed during the Eta filtering step :(\")\n",
    "            else:\n",
    "                print(\"*** Eta filtering step failed :(\")\n",
    "        elif df_filtered_multiplicity is not None and df_filtered_multiplicity.empty:\n",
    "            print(\"*** All events were removed during the jet multiplicity filtering step :(\")\n",
    "        else:\n",
    "            print(\"*** Jet multiplicity filtering step failed :(\")\n",
    "    else:\n",
    "        print(\"*** Data loading failed. Aborting processing :(\")\n",
    "\n",
    "    # 5. Determine Event Label (Improved approach needed)\n",
    "    if 'event_label' in df.columns:\n",
    "         try:\n",
    "             event_label = int(df['event_label'].iloc[0])\n",
    "             print(f\"Determined event label '{event_label}' from 'event_label' column.\")\n",
    "         except (ValueError, TypeError, IndexError):\n",
    "              print(\"Error: Could not determine event label from 'event_label' column. Defaulting to -1.\")\n",
    "              event_label = -1 # Indicate unknown label\n",
    "    elif \"background\" in input_filepath.lower(): # Fallback to filename check (less reliable)\n",
    "        event_label = 0\n",
    "        print(\"Determined event label '0' based on filename (background).\")\n",
    "    elif \"ax\" in input_filepath.lower(): # Add signal check if needed\n",
    "        event_label = 1\n",
    "        print(\"Determined event label '1' based on filename (signal).\")\n",
    "    else:\n",
    "        event_label = -1 # Unknown\n",
    "        print(\"Warning: Could not determine event label from filename or column. Using '-1'.\")\n",
    "\n",
    "\n",
    "    # 6. Process Events\n",
    "    gnn_data_list = []\n",
    "    print(f\"\\nConverting {len(df)} events to GNN format...\")\n",
    "    # Use itertuples for faster iteration than iterrows\n",
    "    # `index=False` gives only the data columns\n",
    "    # `name=None` returns standard tuples (slightly faster)\n",
    "    for event_tuple in tqdm(df.itertuples(index=False, name=None), total=len(df), desc=\"Processing Events\"):\n",
    "        # Convert tuple back to Series with correct index (column names)\n",
    "        event_series = pd.Series(event_tuple, index=df.columns)\n",
    "        gnn_dict = process_event_to_gnn(event_series, event_label, MAX_JETS, MAX_PHOTONS)\n",
    "        if (gnn_dict is not None) and (gnn_dict['num_btag_jets'] == 2) and (gnn_dict['num_isophotons'] == 1):\n",
    "            gnn_data_list.append(gnn_dict)\n",
    "\n",
    "    print(f\"\\nSuccessfully converted {len(gnn_data_list)} events out of {len(df)}.\")\n",
    "\n",
    "    # 7. Save Results\n",
    "    if gnn_data_list:\n",
    "        success = save_to_json(gnn_data_list, output_filepath)\n",
    "        if success:\n",
    "            print(f\"\\nPipeline finished successfully. GNN data saved to {output_filepath}\")\n",
    "            # Optional: Load and print a sample\n",
    "            try:\n",
    "                with open(output_filepath, 'r') as f: sample_data = json.load(f)\n",
    "                if sample_data: print(\"\\n--- Sample GNN Event:\\n\", json.dumps(sample_data[0], indent=2))\n",
    "            except: pass # Ignore errors reading sample\n",
    "        else:\n",
    "            print(\"\\nPipeline finished, but saving the JSON file failed.\")\n",
    "    else:\n",
    "        print(\"\\nPipeline finished, but no events were successfully converted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c19cb3d7-604a-4bf9-85d3-ba523ea40be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 files to preprocess and convert to JSON file: \n",
      "['./raw_txt_data/background_ppbba_500k_minpt10_15jets_etafiltered_corrected.txt', './raw_txt_data/background_ppbba_500k_minpt20_15jets_etafiltered_corrected.txt', './raw_txt_data/ppzaxbba_ax15_200k_minpt10_15jets_etafiltered_corrected.txt', './raw_txt_data/ppzaxbba_ax15_200k_minpt20_15jets_etafiltered_corrected.txt', './raw_txt_data/ppzaxbba_ax45_200k_minpt10_15jets_etafiltered_corrected.txt', './raw_txt_data/ppzaxbba_ax45_200k_minpt20_15jets_etafiltered_corrected.txt']\n",
      "\n",
      "Files will be saved to paths: \n",
      "['./background_ppbba_500k_minpt10_15jets_etafiltered_corrected_onlyFirst2bj_onlyFirst1p_GNN_JSON_data.json', './background_ppbba_500k_minpt20_15jets_etafiltered_corrected_onlyFirst2bj_onlyFirst1p_GNN_JSON_data.json', './ppzaxbba_ax15_200k_minpt10_15jets_etafiltered_corrected_onlyFirst2bj_onlyFirst1p_GNN_JSON_data.json', './ppzaxbba_ax15_200k_minpt20_15jets_etafiltered_corrected_onlyFirst2bj_onlyFirst1p_GNN_JSON_data.json', './ppzaxbba_ax45_200k_minpt10_15jets_etafiltered_corrected_onlyFirst2bj_onlyFirst1p_GNN_JSON_data.json', './ppzaxbba_ax45_200k_minpt20_15jets_etafiltered_corrected_onlyFirst2bj_onlyFirst1p_GNN_JSON_data.json']\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "\n",
      "Starting to process the files...\n",
      "Loading data from ./raw_txt_data/background_ppbba_500k_minpt10_15jets_etafiltered_corrected.txt...\n",
      "--- Data loaded successfully: 500000 events, 129 columns :)\n",
      "Applying Eta filter: Keeping jets with Eta between -2.5 and 2.5.\n",
      "--- Eta filtering complete :)\n",
      "['jet1_Eta', 'jet2_Eta', 'jet3_Eta', 'jet4_Eta', 'jet5_Eta', 'jet6_Eta', 'jet7_Eta', 'jet8_Eta', 'jet9_Eta', 'jet10_Eta', 'jet11_Eta', 'jet12_Eta', 'jet13_Eta', 'jet14_Eta', 'jet15_Eta']\n",
      "Removed 29152 events with no valid jets AND no valid photons :)\n",
      "Number of events after empty event filtering: 470848\n",
      "Determined event label '0' based on filename (background).\n",
      "\n",
      "Converting 470848 events to GNN format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Events: 100%|██████████| 470848/470848 [09:51<00:00, 795.58it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully converted 5645 events out of 470848.\n",
      "Attempting to save 5645 events to JSON file: ./background_ppbba_500k_minpt10_15jets_etafiltered_corrected_onlyFirst2bj_onlyFirst1p_GNN_JSON_data.json\n",
      "JSON file saved successfully.\n",
      "\n",
      "Pipeline finished successfully. GNN data saved to ./background_ppbba_500k_minpt10_15jets_etafiltered_corrected_onlyFirst2bj_onlyFirst1p_GNN_JSON_data.json\n",
      "\n",
      "--- Sample GNN Event:\n",
      " {\n",
      "  \"eventno\": 262,\n",
      "  \"event_label\": 0,\n",
      "  \"nodes\": [\n",
      "    [\n",
      "      0.55687,\n",
      "      -0.181863,\n",
      "      45.2844,\n",
      "      52.4892\n",
      "    ],\n",
      "    [\n",
      "      -0.460313,\n",
      "      0.224215,\n",
      "      38.8455,\n",
      "      43.0342\n",
      "    ],\n",
      "    [\n",
      "      -2.304,\n",
      "      -2.48777,\n",
      "      13.1734,\n",
      "      66.618\n",
      "    ],\n",
      "    [\n",
      "      -1.4672,\n",
      "      0.447601,\n",
      "      12.5798,\n",
      "      28.73\n",
      "    ]\n",
      "  ],\n",
      "  \"num_nodes\": 4,\n",
      "  \"edges\": [\n",
      "    1.0952445377964684,\n",
      "    1.0952445377964684,\n",
      "    3.6744774117619774,\n",
      "    3.6744774117619774,\n",
      "    2.119689668842116,\n",
      "    2.119689668842116,\n",
      "    3.2793359684841685,\n",
      "    3.2793359684841685,\n",
      "    1.0313693498281786,\n",
      "    1.0313693498281786,\n",
      "    3.052316685345903,\n",
      "    3.052316685345903\n",
      "  ],\n",
      "  \"edge_index\": [\n",
      "    [\n",
      "      0,\n",
      "      1,\n",
      "      0,\n",
      "      2,\n",
      "      0,\n",
      "      3,\n",
      "      1,\n",
      "      2,\n",
      "      1,\n",
      "      3,\n",
      "      2,\n",
      "      3\n",
      "    ],\n",
      "    [\n",
      "      1,\n",
      "      0,\n",
      "      2,\n",
      "      0,\n",
      "      3,\n",
      "      0,\n",
      "      2,\n",
      "      1,\n",
      "      3,\n",
      "      1,\n",
      "      3,\n",
      "      2\n",
      "    ]\n",
      "  ],\n",
      "  \"node_labels\": [\n",
      "    0,\n",
      "    1,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"jet_btag_label\": [\n",
      "    0,\n",
      "    1.0,\n",
      "    0.0,\n",
      "    1.0\n",
      "  ],\n",
      "  \"num_btag_jets\": 2,\n",
      "  \"num_isophotons\": 1,\n",
      "  \"inv_mass_2j1p\": 133.31956216267588,\n",
      "  \"inv_mass_2j\": 65.15708160303912,\n",
      "  \"isophoton_pT\": 45.2844\n",
      "}\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "All files processed SUCCESSFULLY :)\n"
     ]
    }
   ],
   "source": [
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Define constants for jet features and limits\n",
    "    JET_FEATURES = ['Eta', 'Phi', 'pT', 'Px', 'Py', 'Pz', 'E']\n",
    "    PHOTON_FEATURES = ['Eta', 'Phi', 'pT', 'Px', 'Py', 'Pz', 'E']\n",
    "    BTAG_FEATURE = 'btag' \n",
    "    NODE_FEATURES = ['Eta', 'Phi', 'pT', 'E']\n",
    "    KINEMATIC_FEATURES = ['E', 'Px', 'Py', 'Pz']\n",
    "    POS_FEATURES = ['Eta', 'Phi'] # For DeltaR\n",
    "    \n",
    "    MAX_JETS = 15 # Change this to 2 if you just want to use jet1 and jet2\n",
    "    MAX_PHOTONS = 1 \n",
    "    \n",
    "    ETA_MIN = -2.5\n",
    "    ETA_MAX = 2.5\n",
    "    INPUT_SEPARATOR = '\\t'\n",
    "    BASE_TXT_DATA_DIR = \"./raw_txt_data\"\n",
    "    INPUT_FILE_PATHS = []\n",
    "    for files in os.listdir(BASE_TXT_DATA_DIR):\n",
    "        if files.endswith(\".txt\"):\n",
    "            INPUT_FILE_PATHS.append(os.path.join(BASE_TXT_DATA_DIR, files))\n",
    "    \n",
    "    print(f\"Found {len(INPUT_FILE_PATHS)} files to preprocess and convert to JSON file: \\n{INPUT_FILE_PATHS}\")\n",
    "    \n",
    "    GNN_OUTPUT_JSON_FILE_PATHS = []\n",
    "    # BASE_OUTPUT_DIR = \"./GNN_JSON_DATA/onlyFirst2bj_onlyFirst1p\"\n",
    "    BASE_OUTPUT_DIR = \"./\"\n",
    "    os.makedirs(BASE_OUTPUT_DIR, exist_ok=True)\n",
    "    for paths in INPUT_FILE_PATHS:\n",
    "        if paths.endswith(\".txt\"):\n",
    "            output_path = paths.split(\"/\")[-1][:-4] + '_onlyFirst2bj_onlyFirst1p_GNN_JSON_data.json'\n",
    "            GNN_OUTPUT_JSON_FILE_PATHS.append(os.path.join(BASE_OUTPUT_DIR, output_path))\n",
    "    \n",
    "    print(f\"\\nFiles will be saved to paths: \\n{GNN_OUTPUT_JSON_FILE_PATHS}\")\n",
    "    for _ in range(5):\n",
    "        print(\"*\")\n",
    "    print(f\"\\nStarting to process the files...\")\n",
    "    for input_file, output_file in zip(INPUT_FILE_PATHS, GNN_OUTPUT_JSON_FILE_PATHS):\n",
    "        main_pipeline(\n",
    "            input_filepath=input_file,\n",
    "            output_filepath=output_file,\n",
    "            MAX_JETS=MAX_JETS,\n",
    "            MAX_PHOTONS=MAX_PHOTONS,\n",
    "            ETA_MIN=ETA_MIN,\n",
    "            ETA_MAX=ETA_MAX,\n",
    "            sep=INPUT_SEPARATOR\n",
    "        )\n",
    "        for _ in range(3):\n",
    "            print(\".\")\n",
    "    for _ in range(5):\n",
    "        print(\"*\")\n",
    "    print(f\"All files processed SUCCESSFULLY :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e885517c-2e17-41de-a864-e86969b81a73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
